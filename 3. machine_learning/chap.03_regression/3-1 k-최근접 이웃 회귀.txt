3-1 k-최근접 이웃 회귀

농어를 무게 단위로 판매
=> 농어 무게 재측정 (즉, 예측)

농어의 길이, 높이, 두께 측정 데이터 사용
=> 무게 측정 샘플 56개


k-최근접 이웃 회귀
지도 학습 알고리즘 
분류와 회귀로 나뉜다.
분류 : 몇 개의 클래스중 하나로 분류하는 알고리즘.
회귀 : 임의의 어떤 숫자를 예측하는 알고리즘.
예) 내년도 경제 성장률 예측, 배달 도착 시간 예측 등.

 k-최근접 이웃 분류 알고리즘
1. 예측하려는 샘플에 가까운 샘플 k개를 선택.
2. 이 샘플들의 클래스를 확인하여
3. 다수 클래스를 새로운 샘플의 클래스로 예측.

k-최근접 이웃 회귀 알고리즘
1. 예측하려는 샘플에 가장 가까운 샘플 k개를 선택.
2. 이웃한 샘플의 타깃은 임의의 수치.
   즉, 이웃 샘플의 수치를 사용하여 새로운 샘플 X의 타깃을 예축하는 방법
=> 방법 : 이웃한 샘플의 타깃 값에 대한 평균을 구하면 된다.


데이터 준비 
가정 : 농어의 길이만 있어도 무게를 예측할 수 있다.

import numpy as np

perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )



데이터 분포 확인 (산점도:scatter)
  하나의 특성을 사용하기 때문에 
  x축 : 특성 데이터(문제)
  y축 : 타깃 데이터(정답)

import matplotlib.pyplot as plt

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()



훈련세트와 테스트 세트 분리
from sklearn.model_selection import train_test_split


train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)


print(train_input.shape, test_input.shape)
(42,) (14,)
=> ㅁㅎ두 2차원 배열로 반환됨.

사이킷런의 훈련 세트는 2차원 배열이어야 하기 때문에
=> 넘파이의 reshape()을 이용하여 2차원으로 변형.


reshape() 테스트 : (4, ) => (2, 2)로 변형

test_array = np.array([1,2,3,4])
print(test_array.shape)
(4,)

test_array = test_array.reshape(2, 2)
print(test_array.shape)
(2, 2)

=> 주의 사항 : 지정한 크기가 원본 배열의 요소의 갯수와 다르면 에러 발생.
ValueError: cannot reshape array of size 4 into shape (2,3)

# train_input 첫 번째  크기를 나머지 요소(특성)로 채우고, 두 번재 크기를 1로 지정
train_input = train_input.reshape(-1, 1)

# test_input 첫 번째  크기를 나머지 요소(특성)로 채우고, 두 번재 크기를 1로 지정
test_input = test_input.reshape(-1, 1)

print(train_input.shape, test_input.shape)
(42, 1) (14, 1)




결정 계수(R2)
뷴류의 경우 정확하게 분류한 갯수(정확도:정답을 맞춘 갯수의 비율)
회귀의 경우 예축하는 값이나 타깃 모두 임의의 수치(결정계수로 평가)
0 에 가까울 경우 : 타깃의 평균 정도를 예측하는 수준
1 에 가까울 경우  : 예측이 타깃에 아주 가까워질 경우 

from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()

# k-최근접 이웃 회귀 모델을 훈련합니다
knr.fit(train_input, train_target)

knr.score(test_input, test_target)
0.992809406101064


타깃과 예측한 값 사이의 차이 확인 : mean_absolute_error()
=> 타깃과 예측의 절대값 오차를 평균하여 반환

from sklearn.metrics import mean_absolute_error

# 테스트 세트에 대한 예측을 만듭니다
test_prediction = knr.predict(test_input)

# 테스트 세트에 대한 평균 절댓값 오차를 계산합니다
mae = mean_absolute_error(test_target, test_prediction)

print(mae)
19.157142857142862


훈련 세트를 사용하여 평가 했을 때?
print(knr.score(train_input, train_target))
0.9698823289099254




과대적합 vs 과소적합
모델을 훈련 세트로 훈련하면 =>  훈련 세트에 적합한 모델

모델을 훈련 세트와 테스트 세트로 각각 평가 했을 경우
=> 보통 훈련 세트 점수가 조금 더 높게 나온다.


과대적합(Overfiting) :
만약, 훈련 세트에서 점수가 좋았는데, 테스트 세트에서 굉장히 나쁜 경우
훈련세트에 과대적합.
즉, 
훈련세트에만 잘 맞는 모델이라 테스트 세트와 실전에서 새로운 샘프에 대한 예측이 잘 작동하지 않는다.


과소적합(Underfiting)
훈련 세트와 테스트 세트의 점수가 모두 너무 낮을 경우.
또는 테스트 세트 점수가 훈련 세트 점수보다 높을 경우.
훈련 세트에 과소적합.
즉,
모델이 너문 단순하여 훈련 세트에 적절한 훈련이 되지 않은 경우.
=> 원인 : 데이터가 너무 작을 경우.

해결방법 : 모델을 좀더 복잡하게 설계
설계 방법 : 이웃의 갯수 k를 줄이는 것.
=> 이웃의 갯수를 줄이면 훈련 ㅅ트에 있는 국지적(지역적) 패턴에 민감해지고,
=> 이웃의 갯수를 늘리면 데이터 전반에 있는 일반적인 패턴을 따르게 된다.

이웃의 갯수를 3으로 설정 후, 다시 훈련

# 이웃의 갯수를 3으로 설정합니다
knr.n_neighbors = 3

# 모델을 다시 훈련합니다
knr.fit(train_input, train_target)

print(knr.score(train_input, train_target))
0.9804899950518966


print(knr.score(test_input, test_target))
0.9746459963987609







데이