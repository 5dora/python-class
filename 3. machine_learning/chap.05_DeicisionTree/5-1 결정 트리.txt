5-1 결정 트리

와인 구별
가정
와인을 특수 캔으로 맛과 향이 유지되도록..
입고된 와인캔에는 화이트와 레드를 구별할 수 있는 표시가 누락.

캔에 인쇄된 도수, 당도, pH 데이터를 이용하여 와인을 구별


1차 해결방안 :  도수, 당도, pH 데이터에 로지스틱 회귀 모델을 적용

로지스틱 회귀로 와인 구별하기
데이터 : 6,497개의 와인 샘플 데이터

데이터 준비
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')

데이터 확인
wine.head()

alcohol : 알코올 도수
sugar : 당도
pH : pH값(포도의 성숙도: 레드 와인은 pH 정도가 높은 와인)
class : 0(레드와인) / 1(화이트와인)

따라서 이진 분류이고
class 값이 1인 화이트 와인이 양성 클래스.
즉, 전체와인에서 화이트 와인만 골라내면 됨.


* 참고
wine.info() : 데이터프레임의 정보플 알고 싶을 때.
데이터프레임에 누락된 정보 알아낼때도 유용하다.

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 6497 entries, 0 to 6496
Data columns (total 4 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   alcohol  6497 non-null   float64
 1   sugar    6497 non-null   float64
 2   pH       6497 non-null   float64
 3   class    6497 non-null   float64
dtypes: float64(4)
memory usage: 203.2 KB

Non-Null Count가 6497이므로 누락데이터는 없다는 것을 확인.

만약, 누락된 데이터가 있을 경우에는 
그 샘플을 버리거나, 평균값으로 채운후에 사용할 수도 있다.


wine.describe() : 기본 통계값 확인.

	alcohol	sugar		pH		class
count	6497.000000	6497.000000	6497.000000	6497.000000
mean	10.491801	5.443235	3.218501	0.753886
std	1.192712	4.757804	0.160787	0.430779
min	8.000000	0.600000	2.720000	0.000000
25%	9.500000	1.800000	3.110000	1.000000
50%	10.300000	3.000000	3.210000	1.000000
75%	11.300000	8.100000	3.320000	1.000000
max	14.900000	65.800000	4.010000	1.000000

4분위수 : 데이터를 순서대로 정렬한 후, 순서대로 4등분한 값.

alcohol과 sugar, pH, class값의 스케일이 다르다는 것을 확인.
따라서 표준화 작업이 필요.

1. class를 정답(타깃)으로 / 나머지를 input(문제)로 
2. 데이터 프레임을 넘파이 배열로 변환.
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()


3. 훈련 세트와 테스트 세트로 분리.
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

print(train_input.shape, test_input.shape)

4 StandardScalar 를  이용하여 표준화.
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)


분리된 데이터 확인
print(train_input.shape, test_input.shape)
(5197, 3) (1300, 3)


로지스틱 회귀 모델로 훈련.
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

0.7808350971714451
0.7776923076923077
=> 과소적합 발생

모델 설명을 위한 학습 계수와 절편을 확인.
print(lr.coef_, lr.intercept_)
[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]

알코올 도수와 당도가 높을 수록 화이트와인
pH가 높을수록 레드와인

따라서
pH가 어떤 기준 값보다 크면 레드와인
그렇지 않으면 화이트와인



결정트리 알고리즘 
노드에서 최적의 분할을 찾기 전에 특성의 순서를 섞는다.
따라서 약간의 무작위성이 주입되기 때문에 실행할 때마다 점수에 변화가 생김.
실전에서는 random_state를 지정하지 않지만,
결과를 동일하게 만들기 위해 random_state를 지정하여 실습.

* 참고 
노드(node) : 결정트리를 구성하는 핵심 요소
                    훈련데이터의 특성을 텍스트로 표현
가지(branch) : 테스트 결과(Ture/False)를 나타냄
일반적으로 하나의 노드는 두 개의 브런치를 가짐.

결정트리는 위에서 아래로..
맨 위의 노드를 root node.
맨 아래 끝에 달린 노르를 leaf node

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

0.996921300750433
0.8592307692307692



결정트리 모델을 시각화 : 사이킷런의 plot_tree()

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()



일부만 시각화 :  plot_tree()
max_depth : 트리의 깊이를 제한
예) 1 : 루트 노드를 제외하고 하나의 노드를 더 확장하여 시각화.

filled : True일 경우 클래스에 맞게 노드의 색을 자동 변경 

feature_names : 특성의 이름을 리스트 형태로 전달.

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()



gini (지니 불순도:Gini impurity)
클래스의 비율을 제곱하여 더한 다음 1에서 뺀 값
DecisionTreeClassifier의 매개변수인 criterion의 기본 값이 'gini'
criterion 매개변수의 역할 : 노드엣 데이터를 분할할 기준을 정함.

결정트리 모델은 
부모노드(parent node)와 자식노드(child node)의 불순도 차이가 가능한 크도록 트리를 생성.

정보이득(Information gain) : 부모노드와 자식노드의 불순도 차이.

엔트로피 불순도(Entropy impurity)
클래스 비율을 사용하지만 밑이 2인 로그를 곱한다.



가지치기 : 자라날수 있는 트리의 최대 깊이을 지정.
DecisionTreeClassifier의 매개변수인 max_depth에 값을 지정.

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

0.8454877814123533
0.8415384615384616


plot_tree() 로 시각화
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()


결정 트리는 표준화 전처리가 필요 없다!!!!!

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

0.8454877814123533
0.8415384615384616


plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()


특성 중요도 확인
print(dt.feature_importances_)
 알코올도수       당도           pH
[0.12345626 0.86862934 0.0079144 ]

특성 중요도 계산방법
각 노드의 정보이득과
전체샘플에 대한 비율을 곱한 후,
특성별로 더한다.














