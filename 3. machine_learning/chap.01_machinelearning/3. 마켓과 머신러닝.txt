3. 마켓과 머신러닝

가장 간단한 머신러닝 알고리즘 중, 
k-최근접 이웃(Nearest Neighbors)을 사용하여 2개의 종류를 분류하는 머신러닝 모델(Model)을 훈련.

* 머신러닝에서 
  여러 개의 종류(class) 중 하나를 구별해내는 문제를 분류(classification)
  2개의 클래스 중 하나를 고르는 문제를 이진 분류(Binary classification)
  데이터의 특징을 특성(Feature)

 
생선 분류 문제 (도미와 빙어 분류)

일반 프로그램 : 누군가 정해준 기준대로 일을 함.
머신러닝 : 누구도 알려주지 않는 기준을 찾아서 일을 함.

예) 일반적으로 도미는 30~40cm 길이.
일반 프로그램
if 30 <= fish_length or 40 >= fish_length :
   print('도미')

머신러닝
'30~40cm 길이의 생선은 도미이다' 라는 기준을 찾아
이 기준을 이용하여 도미인지 아닌 지를 판별.


도미 데이터 35개 준비하기

도미의 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]

도미의 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

도미의 특징(특성:feature)을 길이와 무게로 설정

도미 데이터 시각화(산점도: scatter plot)
x축 : 도미 길이 / y축 : 도미 무게
import matplotlib.pyplot as plt

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
=> 도미의 길이가 길수록 무게가 많이 나가는 형태.

선형(Linear)적 데이터 : 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우


빙어 데이터 14개 준비하기

빙어의 길이
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

빙어의 무게
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]


도미와 빙어 데이터 시각화(산점도: scatter plot)
x축 : 생선 길이 / y축 : 생선 무게
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
=> 도미의 길이가 길수록 무게가 많이 나가는 형태.
     길이와 무게가 비례하여 무게가 길이에 영향을 받는다.
=> 빙어는 길이가 늘어나더라도 무게가 많이 늘지는 않음.
     무게가 길이에 영향을 덜 받는다.



첫 번째 머신러닝

k-최근접 이웃(Nearest Neighbors) 알고리즘을 사용하여 도미와 빙어 데이터를 구분.

1. 도미의 길이 데이터와 빙어의 길이 데이터를 하나의 리스트로.
   length = bream_length+smelt_length

2. 도미의 무게 데이터와 빙어의 무게 데이터를 하나의 리스트로.
   weight = bream_weight+smelt_weight

3. 사이킷런(scikit-learn) 사용을 위해서는
   각 특성(feature)의 리스트를 세로 방향([[무게, 길이],..., [무게, 길이]] )으로 변형해야 함.
   즉, 2차원 리스트로 변형.

fish_data = [[l, w] for l, w in zip(length, weight)]
print(fish_data)


* zip() : 두 개 이상의 리스트 각각에서 하나의 원소를 꺼내어 반환.


정답 데이터 준비하기

머신러닝 및 컴푸터 프로그램은 숫자만 인식.
도미와 빙어를 숫자(1, 0)로 표현.

도미와 빙어를 순서대로 35개, 1개로 나열했지 때문에
정답 리스트는 1이 35번, 0이 14번 등장하면 됨.
fish_target = [1]*35 + [0]*14
print(fish_target)

* 머신러닝에서 2개를 구붕하는 경우,
  찾으려는 대상을 1, 그 외는 0으로 정의.
  따라서 도미를 찾기 위해 도미는 1, 빙어는 0으로 설정


k-최근접 이웃(Nearest Neighbors) 알고리즘 구현 클래스 
사이킷런 서브 패키지의 KNeighborsClassifier

1. 클래스 import
from sklearn.neighbors import KNeighborsClassifier

2. KNeighborsClassifier 객체 생성
kn = KNeighborsClassifier()

3. 도미를 찾기 위한 기준을 학습 : fit()
   생성된 kn 객체의 fit() 메서드에 훈련 데이터와 정답 데이터를 전달하여 훈련(Training)
kn.fit(fish_data, fish_target)

4. 훈련된 객체(모델:Model)를 평가 : score()
   score() 메서드는 0~1사이의 값을 반환
   1은 100% 맞혔다는 의미. 0.5는 50%만 맞혔다는 의미.
kn.score(fish_data, fish_target)

* score() 의 결과가 array([0]) 또는 array([1])로 나오는 이유는
  2진 분류가 아닐 경우에는 여러 개의 정답이 만들어질 수 있기 때문.



k-최근접 이웃(Nearest Neighbors) 알고리즘 
어떤 데이터에 대한 답을 구할 때, 주위의 다른 데이터를 보고 다수를 ㅏ지하는 것을 정답으로 사용.
즉, 주위의 데이터를 보고 현재 데이터를 판단하는 것.

새로운 데이터의 정답 예측 : predict()
새로운 데이터에 대한 예측을 할 때는 가장 가까운 직선거리에 어떤 데이터가 있는지를 살피면 된다.

단점 : 데이터가 아주 많을 경우, 
메모리가 많이 필요하고와 직선거리를 계산하는 시간이 많이 걸린다.

KNeighborsClassifier의 _fit_X : 훈련 데이터
KNeighborsClassifier의 _fit_y : 정답 데이터


k-최근접 이웃(Nearest Neighbors) 알고리즘은 
KNeighborsClassifier의 fit() 메서드에 전달한 데이터를 모두 저장하고 있다가 
새로운 데이터가 등장하면 가장 가까운 데이터를 참고하여 도미인지 빙어인지를 구분.

참고할 가까운 데이터의 갯수는 기본값이 5.
기본값을 변경할 경우 : KNeighborsClassifier 객체 생성할 때, n_neighbors 매개변수 값을 설정
kn49 = KNeighborsClassifier(n_neighbors=49)
참고할 데이터를 49개로 설정.

모델에 fish_data(훈련데이터)를 적용하면
fish_data의 데이터 49개 중 도미가 35개로 다수를 차지하므로 어떤 데이터를 넣어도 무조건 도미로 인식.


참고
matplotlib의 scatter() : 산점도 
  scatter(x축 데이터 리스트, y축 데이터 리스트)
  색상 : c = '#RGB 16진수' 또는 c='색상코드'
          RGB : #RRGGBB
          색상코드 : r(빨강), b(파랑), g(초록), c(하늘) m(핑크), y(노랑), k(검정), w(하양)
  기본색상 : https://bit.ly/matplotlib_prop_cycle
 
  모양 변경 : marker = '마커스타일'  
         기본값은 o(circle : 원모양)
         마커종류 : https://bit.ly/matplotlib_marker


scikit-learn의 KNeighborsClassifier()
   n_neighbors : 참고할 이웃 데이터의 갯수.
   p : 직선거리 계산 방식 (기본값은 2)
       1 : 맨해튼 거리(Manhattan distance, https://bit.ly/man_distance)
       2 : 유클리드 거리(Euclidean distance, https://bit.ly/euc_distance)
   n_jobs : 사용할 CPU 코어의 갯수 (기본값은 1)
       -1 : 모든 CPU 코어를 사용.
       단, 이웃간의 거리계산 속도는 높일 수 있지만, fit() 메서드에는 영향이 없다)

fit() : 모델을 훈련할 때 사용. 
       훈련 데이터(특성)와 정답 데이터를 전달.

predict() : 모델을 훈련한 후, 예측할 때 사용. 
             특성 데이터 하나만 매개변수로 전달

score() : 훈련된 모델으 성능을 측정할 때 사용. 
           훈련 데이터(특성)와 정답 데이터를 전달.
           먼저 predict() 메서드로 예측을 수행한 후,
           분류모델일 경우 정답과 비교하여 올바르게 예측한 갯수의 비율(0~1)을 반환.






